<span style='font-size: 12px; font-weight: normal; opacity: 0.6;'>Task Type</span>,<span style='visibility: hidden;'>dummy</span>,,,,,,,<a id=8 href='https://euroeval.com/tasks/sentiment-classification/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Sentiment classification</a>,<a id=9 href='https://euroeval.com/tasks/named-entity-recognition/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Named entity recognition</a>,<a id=10 href='https://euroeval.com/tasks/linguistic-acceptability/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Linguistic acceptability</a>,<a id=11 href='https://euroeval.com/tasks/reading-comprehension/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Reading comprehension</a>,<span style='visibility: hidden;'>hidden</span>,,,
model,generative_type,rank,parameters,vocabulary_size,context,commercial,merge,<a href='https://euroeval.com/datasets/latvian#latvian-twitter-sentiment'>latvian_twitter_sentiment</a>,<a href='https://euroeval.com/datasets/latvian#fullstack-ner-lv'>fullstack_ner_lv</a>,<a href='https://euroeval.com/datasets/latvian#scala-lv'>scala_lv</a>,<a href='https://euroeval.com/datasets/latvian#multi-wiki-qa-lv'>multi_wiki_qa_lv</a>,latvian_twitter_sentiment_version,fullstack_ner_lv_version,scala_lv_version,multi_wiki_qa_lv_version
<a href='https://hf.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506'>mistralai/Mistral-Small-3.2-24B-Instruct-2506</a>,📝,1.00@@1.00,24011361280,131072,?,✓,✗,51.25 ± 2.43 / 66.91 ± 1.79@@51.25,61.71 ± 2.22 / 45.90 ± 1.87@@61.71,30.29 ± 1.59 / 62.86 ± 1.43@@30.29,72.50 ± 2.64 / 52.22 ± 3.67@@72.50,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/Qwen/Qwen3-14B'>Qwen/Qwen3-14B</a>,🤔,1.22@@1.22,14768307200,151936,40960.0,✓,✗,45.98 ± 1.35 / 64.33 ± 0.87@@45.98,53.13 ± 2.61 / 36.74 ± 2.89@@53.13,33.15 ± 2.97 / 63.70 ± 3.47@@33.15,67.91 ± 1.47 / 46.14 ± 1.42@@67.91,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/Qwen/Qwen3-4B'>Qwen/Qwen3-4B</a>,🤔,1.99@@1.99,4022468096,151936,40960.0,✓,✗,29.52 ± 5.85 / 37.48 ± 3.78@@29.52,52.84 ± 2.14 / 32.34 ± 1.75@@52.84,12.51 ± 4.14 / 44.54 ± 6.49@@12.51,59.64 ± 1.08 / 38.94 ± 1.00@@59.64,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/Qwen/Qwen3-8B'>Qwen/Qwen3-8B</a>,🤔,2.07@@2.07,8190735360,151936,40960.0,✓,✗,28.24 ± 5.06 / 36.51 ± 4.30@@28.24,61.86 ± 1.24 / 40.88 ± 1.86@@61.86,0.00 ± 0.00 / 33.30 ± 0.27@@0.00,62.69 ± 1.77 / 42.65 ± 1.97@@62.69,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/mistralai/Voxtral-Mini-3B-2507'>mistralai/Voxtral-Mini-3B-2507</a>,📝,2.23@@2.23,4676271104,131072,?,✓,✗,24.48 ± 2.49 / 45.68 ± 3.39@@24.48,42.11 ± 2.80 / 26.95 ± 2.62@@42.11,5.99 ± 1.99 / 50.30 ± 3.08@@5.99,64.57 ± 2.14 / 46.77 ± 2.69@@64.57,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/Qwen/Qwen2.5-1.5B-Instruct'>Qwen/Qwen2.5-1.5B-Instruct</a>,📝,2.54@@2.54,1543714304,151936,32768.0,✓,✗,26.38 ± 0.97 / 39.41 ± 0.88@@26.38,27.38 ± 1.24 / 15.63 ± 0.99@@27.38,3.65 ± 1.93 / 44.88 ± 5.18@@3.65,48.85 ± 0.94 / 32.29 ± 1.28@@48.85,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/Qwen/Qwen3-1.7B'>Qwen/Qwen3-1.7B</a>,🤔,2.97@@2.97,2031739904,151936,40960.0,✓,✗,0.00 ± 0.00 / 13.86 ± 0.25@@0.00,36.82 ± 1.36 / 23.93 ± 1.18@@36.82,2.06 ± 2.48 / 40.92 ± 5.27@@2.06,43.91 ± 1.09 / 26.07 ± 1.28@@43.91,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/HuggingFaceTB/SmolLM2-360M-Instruct'>HuggingFaceTB/SmolLM2-360M-Instruct</a>,📝,3.51@@3.51,361821120,49152,?,✓,✗,3.09 ± 2.38 / 23.49 ± 2.50@@3.09,11.38 ± 2.53 / 10.72 ± 2.59@@11.38,-0.29 ± 1.36 / 33.46 ± 0.44@@-0.29,4.06 ± 1.16 / 0.41 ± 0.30@@4.06,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/google/gemma-3-270m-it'>google/gemma-3-270m-it</a>,📝,3.52@@3.52,268098176,262144,32768.0,✓,✗,6.47 ± 3.47 / 25.85 ± 4.32@@6.47,13.74 ± 1.69 / 12.68 ± 1.41@@13.74,0.25 ± 1.57 / 36.25 ± 3.43@@0.25,3.26 ± 0.99 / 0.14 ± 0.15@@3.26,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/google/gemma-3-270m'>google/gemma-3-270m</a>,🧠,3.52@@3.52,268098176,262144,32768.0,✓,✗,0.70 ± 1.38 / 20.23 ± 1.16@@0.70,17.30 ± 1.59 / 16.26 ± 1.43@@17.30,0.00 ± 0.00 / 33.36 ± 0.27@@0.00,4.89 ± 1.27 / 0.65 ± 0.24@@4.89,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/HuggingFaceTB/SmolLM2-360M'>HuggingFaceTB/SmolLM2-360M</a>,🧠,3.62@@3.62,361821120,49152,?,✓,✗,2.23 ± 1.55 / 25.25 ± 3.74@@2.23,10.10 ± 1.28 / 9.61 ± 1.17@@10.10,0.00 ± 0.00 / 33.30 ± 0.27@@0.00,3.47 ± 1.55 / 0.51 ± 0.38@@3.47,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/HuggingFaceTB/SmolLM2-135M'>HuggingFaceTB/SmolLM2-135M</a>,🧠,3.64@@3.64,134515008,49152,?,✓,✗,-0.50 ± 2.76 / 23.04 ± 2.98@@-0.50,9.60 ± 1.70 / 8.95 ± 1.53@@9.60,0.75 ± 1.62 / 36.28 ± 4.15@@0.75,3.71 ± 0.92 / 0.14 ± 0.10@@3.71,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
<a href='https://hf.co/HuggingFaceTB/SmolLM2-135M-Instruct'>HuggingFaceTB/SmolLM2-135M-Instruct</a>,📝,3.68@@3.68,134515008,49152,?,✓,✗,0.08 ± 0.89 / 17.47 ± 3.15@@0.08,8.08 ± 1.56 / 8.38 ± 1.31@@8.08,0.19 ± 0.72 / 38.37 ± 4.41@@0.19,3.56 ± 0.50 / 0.04 ± 0.05@@3.56,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600,15.16.0.dev0@@151600
