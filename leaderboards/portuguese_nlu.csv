<span style='font-size: 12px; font-weight: normal; opacity: 0.6;'>Task Type</span>,<span style='visibility: hidden;'>dummy</span>,,,,,,,<a id=8 href='https://euroeval.com/tasks/sentiment-classification/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Sentiment classification</a>,<a id=9 href='https://euroeval.com/tasks/named-entity-recognition/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Named entity recognition</a>,<a id=10 href='https://euroeval.com/tasks/linguistic-acceptability/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Linguistic acceptability</a>,<a id=11 href='https://euroeval.com/tasks/reading-comprehension/' style='font-size: 12px; font-weight: normal; color: Grey; text-decoration: underline;'>Reading comprehension</a>,<span style='visibility: hidden;'>hidden</span>,,,
model,generative_type,rank,parameters,vocabulary_size,context,commercial,merge,<a href='https://euroeval.com/datasets/portuguese#sst2-pt'>sst2_pt</a>,<a href='https://euroeval.com/datasets/portuguese#harem'>harem</a>,<a href='https://euroeval.com/datasets/portuguese#scala-pt'>scala_pt</a>,<a href='https://euroeval.com/datasets/portuguese#multi-wiki-qa-pt'>multi_wiki_qa_pt</a>,sst2_pt_version,harem_version,scala_pt_version,multi_wiki_qa_pt_version
<a href='https://hf.co/mistralai/Mistral-Small-3.1-24B-Base-2503'>mistralai/Mistral-Small-3.1-24B-Base-2503</a>,🧠,1.00@@1.00,24011361280.0,131072.0,?,✓,✗,85.01 ± 0.76 / 92.47 ± 0.39@@85.01,59.54 ± 2.64 / 54.86 ± 2.28@@59.54,26.95 ± 3.14 / 59.99 ± 2.83@@26.95,78.99 ± 1.56 / 61.43 ± 2.30@@78.99,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/NovaSky-AI/Sky-T1-32B-Preview'>NovaSky-AI/Sky-T1-32B-Preview</a>,📝,1.17@@1.17,32763876352.0,152064.0,32768.0,✓,✗,82.26 ± 1.28 / 90.85 ± 0.70@@82.26,55.45 ± 1.59 / 47.90 ± 1.47@@55.45,30.16 ± 3.98 / 55.41 ± 4.27@@30.16,78.11 ± 0.76 / 55.44 ± 1.23@@78.11,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/deepcogito/cogito-v1-preview-qwen-32B'>deepcogito/cogito-v1-preview-qwen-32B</a>,📝,1.19@@1.19,32759790592.0,151665.0,131072.0,✓,✗,85.00 ± 0.60 / 92.49 ± 0.30@@85.00,60.12 ± 2.01 / 54.79 ± 1.27@@60.12,26.40 ± 3.38 / 52.06 ± 3.38@@26.40,79.44 ± 0.95 / 60.60 ± 1.32@@79.44,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503'>mistralai/Mistral-Small-3.1-24B-Instruct-2503</a>,📝,1.21@@1.21,24011361280.0,131072.0,?,✓,✗,83.96 ± 0.95 / 91.90 ± 0.51@@83.96,60.73 ± 2.15 / 56.39 ± 1.97@@60.73,22.48 ± 3.96 / 58.19 ± 2.54@@22.48,77.38 ± 1.50 / 55.65 ± 2.39@@77.38,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/deepcogito/cogito-v1-preview-qwen-14B'>deepcogito/cogito-v1-preview-qwen-14B</a>,📝,1.43@@1.43,14765947904.0,151665.0,131072.0,✓,✗,84.58 ± 0.88 / 92.26 ± 0.45@@84.58,52.44 ± 2.20 / 44.37 ± 1.30@@52.44,31.16 ± 3.74 / 58.12 ± 3.75@@31.16,74.50 ± 0.87 / 52.60 ± 0.93@@74.50,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/bullerwins/Magistral-Small-2506-fp8'>bullerwins/Magistral-Small-2506-fp8</a>,🧠,1.55@@1.55,23575680000.0,131072.0,32768.0,✓,✗,83.29 ± 0.73 / 91.59 ± 0.38@@83.29,57.75 ± 2.51 / 50.68 ± 1.97@@57.75,20.04 ± 4.45 / 54.98 ± 4.43@@20.04,76.41 ± 1.58 / 56.23 ± 2.11@@76.41,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/google/gemma-3-27b-pt'>google/gemma-3-27b-pt</a>,🧠,1.74@@1.74,27432406640.0,262144.0,?,✓,✗,82.99 ± 1.03 / 91.45 ± 0.54@@82.99,50.58 ± 1.74 / 45.52 ± 2.17@@50.58,21.16 ± 2.95 / 58.54 ± 2.26@@21.16,76.95 ± 1.23 / 58.29 ± 1.59@@76.95,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/google/gemma-3-27b-it'>google/gemma-3-27b-it</a>,📝,1.85@@1.85,27432406640.0,262144.0,?,✓,✗,82.75 ± 1.07 / 91.25 ± 0.56@@82.75,49.80 ± 2.70 / 43.76 ± 1.53@@49.80,19.55 ± 2.97 / 56.56 ± 2.43@@19.55,71.48 ± 0.96 / 47.38 ± 1.34@@71.48,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/meta-llama/Llama-3.1-405B-Instruct-FP8'>meta-llama/Llama-3.1-405B-Instruct-FP8</a>,📝,1.93@@1.93,405868625920.0,128256.0,131072.0,✓,✗,85.00 ± 0.42 / 92.48 ± 0.20@@85.00,51.31 ± 2.44 / 48.31 ± 1.95@@51.31,18.35 ± 2.20 / 50.87 ± 2.43@@18.35,74.19 ± 1.43 / 51.63 ± 2.48@@74.19,15.12.0@@151200,15.12.0@@151200,15.12.0@@151200,15.12.0.dev0@@151200
"<a href='https://platform.openai.com/docs/models/o4-mini'>o4-mini-2025-04-16 (zero-shot, val)</a>",🤔,2.12@@2.12,?,?,200000.0,✓,✗,83.12 ± 1.56 / 91.46 ± 0.79@@83.12,46.50 ± 2.72 / 31.34 ± 2.49@@46.50,22.26 ± 2.30 / 44.43 ± 2.01@@22.26,69.24 ± 1.26 / 43.87 ± 1.31@@69.24,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
"<a href='https://docs.anthropic.com/en/docs/about-claude'>claude-3-5-sonnet-20241022 (zero-shot, val)</a>",📝,2.37@@2.37,?,?,200000.0,✓,✗,80.16 ± 1.90 / 89.54 ± 1.09@@80.16,46.94 ± 2.17 / 36.01 ± 1.98@@46.94,17.27 ± 2.73 / 44.42 ± 2.03@@17.27,66.21 ± 0.72 / 33.38 ± 1.22@@66.21,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300,15.13.0@@151300
<a href='https://hf.co/AI-Sweden-Models/ModernBERT-large'>AI-Sweden-Models/ModernBERT-large</a>,🔍,4.70@@4.70,394781696.0,50368.0,8192.0,✓,✗,14.71 ± 4.01 / 54.19 ± 3.14@@14.71,39.77 ± 2.87 / 50.09 ± 2.50@@39.77,6.66 ± 3.48 / 49.62 ± 3.59@@6.66,25.81 ± 2.35 / 18.43 ± 1.97@@25.81,15.12.0.dev0@@151200,15.12.0.dev0@@151200,15.12.0.dev0@@151200,15.12.0.dev0@@151200
